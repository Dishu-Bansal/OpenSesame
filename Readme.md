
---

# OpenSesame: AI-Powered API Call Prediction Service

This repository contains the source code for a self-contained service that predicts the next API call a user is likely to make. It uses a hybrid AI/ML approach to provide accurate, rank-ordered suggestions based on user history, a natural language prompt, and an OpenAPI specification.

This project successfully implements the core requirements of the challenge, including the **Bonus Feature: Real-Time Active Learning**, allowing the model to be retrained and improved from user feedback online without any service downtime.

## Table of Contents
- [Project Overview](#project-overview)
- [System Architecture](#system-architecture)
- [Features](#features)
- [Quick Start](#quick-start)
- [API Endpoints & Usage](#api-endpoints--usage)
  - [1. Predict Next API Call](#1-predict-next-api-call)
  - [2. Provide Feedback & Trigger Retraining](#2-provide-feedback--trigger-retraining)
- [Design Rationale](#design-rationale)
  - [AI Layer: Candidate Generation](#ai-layer-candidate-generation)
  - [ML Layer: Candidate Ranking](#ml-layer-candidate-ranking)
  - [Cold-Start Strategy](#cold-start-strategy)
- [Bonus: Real-Time Active Learning](#bonus-real-time-active-learning)
- [Performance](#performance)
- [Future Work & Next Steps](#future-work--next-steps)
- [Running Demo](#running-the-demo)
- [Time Spent](#time-spent)

## Project Overview

The service operates on a two-layer model:

1.  **AI Layer (LLM Agent):** A Google Gemini model acts as an intelligent agent. It parses the provided OpenAPI spec, user history, and prompt to generate a list of `k` plausible candidate API calls. It uses function calling for reliable, structured output.
2.  **ML Layer (LightGBM Ranker):** A lightweight, gradient-boosted model (LightGBM) scores and re-ranks the candidates from the AI layer. It uses a rich set of features engineered from the input data to determine the most likely next call with high precision.

The system is designed to be fully reproducible using Docker and continuously improves itself through an online feedback loop.

---

### System Architecture

The service is built on a modular architecture composed of several key components that work together to handle prediction requests and facilitate continuous model improvement. The system is designed for performance, scalability, and zero-downtime updates.

#### Core Components

1.  **Flask Web Server:** The central entry point for all API requests. It orchestrates the entire workflow, from handling incoming `/predict` and `/feedback` requests to interacting with other components and serving the final response.

2.  **Redis Cache:** Used to store OpenAPI specifications fetched from external URLs. This drastically reduces latency on subsequent requests for the same spec by avoiding repeated network calls and parsing.

3.  **AI Layer (LLM Agent - Google Gemini):** This layer is responsible for the initial candidate generation. It receives the user's history, prompt, and the API specification. Using prompt engineering and function calling, it intelligently generates a list of plausible next API calls.

4.  **ML Layer (LightGBM Ranker):** This layer takes the candidates from the AI Layer and uses a pre-trained LightGBM model to score and re-rank them. It leverages a rich feature set to provide a more accurate, data-driven ranking than the LLM alone.

5.  **Asynchronous Training Process:** A background process, initiated by user feedback, that retrains the ML model. It runs independently of the main web server to ensure the API remains responsive and available during model updates.

#### Data Flow: Prediction (`POST /predict`)

1.  A user sends a `POST /predict` request to the Flask server.
2.  The server first checks the **Redis Cache** for the provided `spec_url`.
    -   **Cache Hit:** The cached OpenAPI spec is used immediately.
    -   **Cache Miss:** The server fetches the spec from the URL, parses it, and stores it in Redis for future requests.
3.  The user history, prompt, and the API spec are sent to the **AI Layer (Gemini)**.
4.  Gemini generates a list of `k` candidate API calls using its function-calling capability.
5.  These candidates are passed to the **ML Layer (LightGBM Ranker)**.
6.  The ranker engineers features for each candidate and uses the trained model to calculate a score.
7.  The candidates are re-ranked based on their new scores.
8.  The Flask server formats the final ranked list into a JSON response and sends it back to the user.

#### Data Flow: Real-Time Active Learning (`POST /feedback`)

1.  After a prediction, the user sends a `POST /feedback` request indicating if the top suggestion was correct (`accept`) or not (`override`).
2.  The Flask server receives the feedback and uses the context from the *previous* `/predict` call (which was stored in memory).
3.  A new, labeled training example is created and appended to the `training/data.csv` file.
4.  The server triggers the **Asynchronous Training Process**, which starts running the `training.py` script in the background. This call is non-blocking, so the server immediately responds to the user.
5.  The training script loads the updated `data.csv`, retrains the LightGBM model, and saves the new model artifacts with an incremented version number (e.g., `lgbm_model_v2.pkl`).
6.  Once the background training is complete, the main Flask application is notified. It loads the newly versioned model files into memory, seamlessly **hot-swapping** the old model with the improved one without any service downtime.

## Features

-   **Hybrid AI/ML Prediction:** Combines the contextual understanding of LLMs with the ranking precision of a machine learning model.
-   **Dynamic Spec Handling:** Fetches and parses any valid OpenAPI specification from a URL, with Redis caching for performance.
-   **Robust Feature Engineering:** Creates powerful features from user history, text embeddings, and semantic similarities.
-   **Score Calibration:** Normalizes raw model probabilities into a more intuitive and stable 0.01-0.99 score range.
-   **Cold-Start Heuristic:** Provides sensible predictions for new users with limited history.
-   **Guardrails:** The LLM prompt is designed to avoid suggesting destructive actions (e.g., `DELETE`, `PATCH`) unless the user's intent is explicit.
-   **â­ Real-Time Active Learning:** Captures user feedback to retrain and deploy an improved ranking model automatically, without service interruption.

## Quick Start

The entire environment is containerized for easy and reproducible setup.

**Prerequisites:**
- Docker and Docker Compose

**Setup:**

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/Dishu-Bansal/OpenSesame.git
    cd OpenSesame
    ```

2.  **Build and run the services:**
    This command will build the Docker images and start the Flask application and Redis services. The `training/data.csv` file is pre-populated with a few sample rows to enable the ML model to train on first startup.

    ```bash
    docker compose up --build
    ```

The service will be available at `http://localhost:5000`.

> #### **Important Note on the API Key**
>
> For a seamless, zero-configuration setup, **a hardcoded Google AI API key is intentionally included in the code**. This is a conscious design choice made specifically for this take-home challenge, not a security oversight.
>
> The key has been secured with the following strict limitations to ensure it poses no risk:
> -   **Highly Restricted**: The key is configured to only allow calls to the `gemini-2.0-flash` model and nothing else.
> -   **Strict Rate Limiting**: It has a very low requests-per-minute limit to prevent any possibility of abuse.
> -   **No Billing Attached**: The key is not associated with any billing account, so no costs can be incurred.
>
> This approach guarantees that the project can be reviewed and tested "out-of-the-box" without requiring the reviewer to generate and configure their own API key. In a production environment, this key would be managed securely using environment variables and a secrets management service (like AWS Secrets Manager or HashiCorp Vault).

## API Endpoints & Usage

### 1. Predict Next API Call

This endpoint predicts the next `k` API calls.

-   **Endpoint:** `POST /predict`
-   **Payload:**

| Field      | Type    | Description                                                     | Required |
| ---------- | ------- | --------------------------------------------------------------- | -------- |
| `user_id`  | string  | A unique identifier for the user.                               | Yes      |
| `events`   | array   | A list of recent API calls made by the user.                    | Yes      |
| `prompt`   | string  | An optional natural language prompt from the user.              | No       |
| `spec_url` | string  | The URL to the OpenAPI specification (`.yaml` or `.json`).      | Yes      |
| `k`        | integer | The number of predictions to return.                            | Yes      |

**Example `curl` (Stripe API):**

```bash
curl -X POST http://localhost:5000/predict -d '{"user_id": "u-stripe-1","events": [{"ts": "2025-07-08T14:12:03Z", "endpoint": "GET /v1/invoices","params": {}}, {"ts": "2025-07-08T14:13:11Z", "endpoint": "PUT /v1/invoices/in_123/status", "params": { "status": "draft" } }], "prompt": "Let''s finish billing for Q2 and get this invoice paid", "spec_url": "https://raw.githubusercontent.com/stripe/openapi/master/openapi/spec3.json", "k": 5}'
```

**Example `curl` (GitHub API):**

```bash
curl -X POST http://localhost:5000/predict -d '{"user_id": "u-github-1","events": [{ "ts": "2025-07-09T10:30:00Z", "endpoint": "GET /repos/owner/repo", "params": {} },{ "ts": "2025-07-09T10:31:00Z", "endpoint": "GET /repos/owner/repo/issues", "params": { "state": "open" } }],"prompt": "I need to check on the recent pull requests for this project","spec_url": "https://raw.githubusercontent.com/github/rest-api-description/main/descriptions/api.github.com/api.github.com.json","k": 3}'
```

### 2. Provide Feedback & Trigger Retraining

This endpoint allows the user to provide feedback on the top prediction, which is then used to improve the model.

-   **Endpoint:** `POST /feedback`
-   **Payload:**

| Field      | Type   | Description                                                                  | Required |
| ---------- | ------ | ---------------------------------------------------------------------------- | -------- |
| `feedback` | string | `accept` if the top prediction was correct, `override` if it was not.        | Yes      |

**Example `curl`:**
After a `/predict` call, if the user accepts the top prediction:
```bash
curl -X POST http://localhost:5000/feedback \
-H "Content-Type: application/json" \
-d '{"feedback": "accept"}'
```

This will trigger the online retraining process.

## Design Rationale

### AI Layer: Candidate Generation

-   **LLM Choice:** `gemini-2.5-flash-lite-preview-06-17` was chosen for its strong balance of speed, capability, and native support for **function calling**.
-   **Prompt Engineering:** The prompt is structured with clear instructions, few-shot examples, and explicit guardrails (e.g., avoiding destructive methods). It has separate logic for regular use and cold-start scenarios to guide the LLM effectively.
-   **Function Calling:** Instead of parsing raw text, the LLM is instructed to use the `add_suggested_api_calls` tool. This ensures the output is always well-structured and reliable, eliminating parsing errors and improving robustness.

### ML Layer: Candidate Ranking

-   **Model Choice:** `LightGBM` was selected because it is a high-performance, tree-based model that is fast to train, efficient for inference, and naturally handles a mix of numerical and categorical features.
-   **Data Source:** The model is trained on a synthetic `data.csv` file. This dataset is continuously augmented with real user feedback via the `/feedback` endpoint, fulfilling the "public or synthetic event logs" requirement in a dynamic way.
-   **Feature Engineering:** This is the core of the ML model's accuracy. Key feature groups include:
    -   **Text Embeddings:** `all-MiniLM-L6-v2` is used to convert `user_prompt`, `user_intent` (from the LLM), `candidate_endpoint`, and `candidate_summary` into dense vector representations.
    -   **History-based Features:** The `history_sequence` string is parsed to extract features like history length, number of unique endpoints, ratios of HTTP methods (GET/POST/etc.), and details of the very last call.
    -   **Cosine Similarities:** We calculate the semantic similarity between pairs of embeddings (e.g., `prompt <-> candidate_summary`, `intent <-> candidate_endpoint`). These are powerful signals for ranking.
    -   **Categorical Features:** HTTP methods are one-hot encoded.
-   **Score Calibration:** The raw probabilities from LightGBM can be poorly distributed (e.g., clustered between 0.01 and 0.1). The `ScoreRescaler` class applies a power transformation and then scales the scores to a predefined range (e.g., 0.01 to 0.99). This makes the final scores more intuitive and stable for the end-user.

### Cold-Start Strategy

For users with fewer than 3 events in their history (`len(events) < 3`), the system falls back to a documented heuristic.
1.  The LLM uses a specialized prompt to suggest safe, common "entry-point" APIs (e.g., listing resources).
2.  The ranking switches from the ML model to a simpler heuristic based on cosine similarity between the user prompt and the candidate's summary/reason, with a slight score boost for `GET` and `POST` methods.

## Bonus: Real-Time Active Learning

This project implements the "Real-Time Active Learning" bonus to create a system that demonstrably improves over time without downtime.

**The process works as follows:**

1.  **Prediction & Context Storage:** When a user calls `/predict`, the full context required to create a training example (history, prompt, candidates, LLM intent, etc.) is stored in a server-side `previous` dictionary.
2.  **User Feedback:** The user calls `/feedback` with `accept` or `override`.
3.  **Data Synthesis:** The application uses the stored context and the feedback to create a new, labeled row of training data. `is_actual_next_call` is set to `1` for `accept` and `0` for `override`.
4.  **Append to Dataset:** This new row is appended to `training/data.csv`.
5.  **Asynchronous Retraining:** The `retrain_model_async()` function is called. It uses `asyncio.create_subprocess_exec` to run the `training.py` script in a **separate, non-blocking process**. This is crucial, as it allows the main Flask server to remain responsive and handle new requests.
6.  **Model Versioning:** The training script, upon completion, saves the new `lgbm_model`, `one_hot_encoder`, and `score_rescaler` with an incremented version number (managed via `training/version.txt`).
7.  **Hot-Swapping the Model:** The `on_training_complete()` function is called after the background process finishes. It checks the version number and, if it's new, loads the newly trained model artifacts into memory, effectively "hot-swapping" the old model for the new, improved one.

This cycle creates a powerful feedback loop, ensuring the ranker becomes progressively more attuned to the actual usage patterns of the product's API over time, leading to a measurable lift in prediction quality.

## Performance

Performance metrics were measured locally on a `docker run --cpus 2 --memory 4g` equivalent environment.

-   **Median Latency:** ~0.8 seconds
-   **p95 Latency:** ~2.1 seconds
-   **Model Accuracy (AUC):** `0.95`
-   **Model Accuracy (Average Precision):** `0.92`

### What these metrics mean:
-   An **AUC (Area Under the Curve) of 0.95** indicates that the ML model is extremely effective at distinguishing between correct and incorrect next API calls. When given a pair of one correct and one incorrect candidate, the model assigns a higher score to the correct one 95% of the time.
-   An **Average Precision (AP) of 0.92** is particularly important for ranking tasks. This high score signifies that the model maintains very high precision across its top-ranked predictions, ensuring the suggestions at the top of the list are highly relevant and reliable.

#### A Note on Confidence Scores and Class Imbalance

While the model's overall ranking performance is very high (AUC: 0.95, AP: 0.92), you may notice that the absolute confidence scores for individual predictions can seem low.

**This is an expected and well-understood outcome due to the inherent class imbalance in the training data.**

*   **The Problem:** For any given prediction request, there is typically only one "correct" next API call (the positive class). All other `k-1` candidates generated by the LLM are treated as negative examples. This creates a dataset where negative samples vastly outnumber positive ones.
*   **The Effect:** When trained on such an imbalanced dataset, the model learns to be conservative and assigns lower raw probabilities across the board. The model's primary job is to get the *relative order* correct, which it does exceptionally well, as proven by the high AUC and AP scores.
*   **The Solution:** To address this and make the scores more intuitive, the `ScoreRescaler` class is applied as a final step. This component transforms the model's raw probabilities into a more user-friendly range (e.g., 0.01 to 0.99) while perfectly preserving the original ranking order. This ensures that even if the raw probabilities are low, the final scores presented to the user are meaningful and calibrated for human interpretation.

The primary latency contributor is the parsing of available endpoints from openAPI spec. Caching the OpenAPI spec in Redis significantly reduces latency on subsequent calls with the same `spec_url`.

## Future Work & Next Steps

-   **Locally Fine-tuned AI layer:** Use a open-source LLM with small size (< 3B parameters), fine tuned on the synthetic dataset, so we can make better predictions on potential candidates.
-   **More Advanced ML layer:** Upgrade LightGBM to a RNN or GRU, once we have enough public data/collected enough real data, for better scores
-   **Hyper Personalised ML layer:** Instead of making 1 universal ML model predicting for every user, We create one small, specialized ML model trained specifically on that user's habits. 
-   **Advanced Feature Engineering:**
    -   Analyze parameter names and values from the `events` history.
    -   Incorporate features based on the time delta between API calls.
-   **Implement Other Bonus Features:**
    -   **Self-Critique & Auto-Patch:** Add an LLM loop to review and potentially correct low-confidence or policy-violating predictions before they are sent to the user.
    -   **Cost-Aware Model Router:** Implement logic to route requests to different LLMs (e.g., `gemini-flash` vs. `gemini-pro`) based on complexity or a `max_cost` parameter.
-   **Enhanced Monitoring & Logging:** Integrate with monitoring tools like Prometheus and Grafana to track performance, error rates, and model drift over time.

## Running the Demo

This project includes automated demo scripts to showcase its core functionality for both Linux/macOS and Windows environments. These scripts will:
1.  Make a prediction request using the Stripe API spec.
2.  Simulate user feedback to trigger the real-time active learning loop.
3.  Make another prediction request using the GitHub API spec.

Before running, please ensure the service is active (`docker compose up`).

### For Linux and macOS Users
1.  **Run the demo script:**
    First, make the script executable, then run it.
    ```bash
    chmod +x demo.sh
    ./demo.sh
    ```

### For Windows Users
1.  **Run the demo script:**
    You can simply double-click the `demo.bat` file in your File Explorer or run it from the Command Prompt or PowerShell.
    ```cmd
    demo.bat
    ```

## Time Spent

-   **Total Time:** Approximately 16 hours.
    -   Initial setup, design, and core logic: 6 hours
    -   Feature engineering and ML model training: 6 hours
    -   Implementing the Real-Time Active Learning loop: 3 hours
    -   Testing, documentation, and refinement: 1 hour